{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc6b58bd",
   "metadata": {},
   "source": [
    "# Bernard-Arnold first overnight model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fddd7f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "from PIL import Image #image processing library \n",
    "import joblib\n",
    "\n",
    "# tensorflow imports for layers models and optimiser\n",
    "from tensorflow.keras.layers import Input, Reshape, Dropout, Dense, Flatten,BatchNormalization, Activation, ZeroPadding2D, LeakyReLU, UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from gandywarhol.data_gen import CustomDataGen # our custom data loader\n",
    "from tqdm.notebook import tqdm # makes progress bar\n",
    "from tensorflow.keras.utils import plot_model # allows picture view of model to see input and output size of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97f10c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 128 # length and width of image\n",
    "IMAGE_CHANNELS = 3 # image depth (RGB)\n",
    "# Preview image Frame\n",
    "PREVIEW_ROWS = 4 # rows of images to be produced\n",
    "PREVIEW_COLS = 7 # columns of images to be produced therefore 28 images\n",
    "PREVIEW_MARGIN = 4 # 4 pixels between image \n",
    "\n",
    "# how often to save images\n",
    "SAVE_FREQ = 10\n",
    "\n",
    "# number of channels of noise to generate images from\n",
    "NOISE_SIZE = 100\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 4\n",
    "BATCH_SIZE = 2\n",
    "GENERATE_RES = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf6e59a",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Loading the data using our custom data generator, we want the images to be in the form of a list of paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11222fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = ['../raw_data/'+image for image in os.listdir('../raw_data') if image.endswith('jpeg')]\n",
    "train_set = CustomDataGen(path_list, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e717280",
   "metadata": {},
   "source": [
    "## Function Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37910bad",
   "metadata": {},
   "source": [
    "We need lots of different functions to run our model: discriminator, generator, saving images, saving models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ed9eff",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "The discriminator is a binary image classifier CNN. It uses several Conv2D layers and finally flattens and then has a single dense layer.\n",
    "\n",
    "At the end of the function it is combined with an input function. The purpose of the input function is that it allows us to build a model just by knowing the inputs and outputs of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da520810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(image_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2,\n",
    "    input_shape=image_shape, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, padding='same'))\n",
    "    model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(256, kernel_size=3, strides=1, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(512, kernel_size=3, strides=1, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    input_image = Input(shape=image_shape)\n",
    "    validity = model(input_image)\n",
    "    return Model(input_image, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382afba9",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "The generator takes the noise and produces an image. We use UpSampling2d to ensure that our output is the same size as our real images which is the same size as the input to the discriminator (128,128,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0e76855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(noise_size, channels):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4 * 4 * 256, activation='relu',       input_dim=noise_size))\n",
    "    model.add(Reshape((4, 4, 256)))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(256, kernel_size=3, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(256, kernel_size=3, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation('relu'))\n",
    "    for i in range(GENERATE_RES):\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(256, kernel_size=3, padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation('relu'))\n",
    "    model.summary()\n",
    "    model.add(Conv2D(channels, kernel_size=3, padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    input_ = Input(shape=(noise_size,))\n",
    "    generated_image = model(input_)\n",
    "    \n",
    "    return Model(input_, generated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28bdd072",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(generator.layers[1], show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0a2bca",
   "metadata": {},
   "source": [
    "### Saving images and models\n",
    "\n",
    "Saving images, we decide how often we want to save the images and ensure we save them, the same for the model so we can see what it was doing at different times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b4825a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(cnt, noise, epoch):\n",
    "# image_array creates a white rectangle (255) of the size we need it in order to view our 28 images\n",
    "# The margin is the 4 pixels between each image     \n",
    "    image_array = np.full((\n",
    "        PREVIEW_MARGIN + (PREVIEW_ROWS * (IMAGE_SIZE + PREVIEW_MARGIN)),\n",
    "        PREVIEW_MARGIN + (PREVIEW_COLS * (IMAGE_SIZE + PREVIEW_MARGIN)), 3),\n",
    "        255, dtype=np.uint8)\n",
    "# This generates our 28 images, and then puts them between 0 and 1\n",
    "    generated_images = generator.predict(noise)\n",
    "    generated_images = 0.5 * generated_images + 0.5\n",
    "    image_count = 0\n",
    "# The following loop puts each image in its correct square. r takes the pixel on the far left of each square,\n",
    "# c takes the pixel at the top of each square and then starting there the image is placed, this loops through \n",
    "# for each image and its respective square it belongs. \n",
    "    for row in range(PREVIEW_ROWS):\n",
    "        for col in range(PREVIEW_COLS):\n",
    "            r = row * (IMAGE_SIZE + PREVIEW_MARGIN) + PREVIEW_MARGIN\n",
    "            c = col * (IMAGE_SIZE + PREVIEW_MARGIN) + PREVIEW_MARGIN\n",
    "            image_array[r:r + IMAGE_SIZE, c:c +\n",
    "                        IMAGE_SIZE] = generated_images[image_count] * 255\n",
    "            image_count += 1\n",
    "    output_path = 'output'\n",
    "# finally we save the picture of all 28 images in our desired place.\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    filename = os.path.join(output_path, f\"trained-{cnt}.png\")\n",
    "    im = Image.fromarray(image_array)\n",
    "    im.save(filename)\n",
    "    fin_img_path = 'single_imgs'\n",
    "    if not os.path.exists(fin_img_path):\n",
    "        os.makedirs(fin_img_path)\n",
    "    if epoch == EPOCHS:\n",
    "        for i in range(28):\n",
    "            filename = os.path.join(fin_img_path, f\"final-{i}.png\")\n",
    "            im = Image.fromarray(generated_images[i]*255)\n",
    "            im.save(filename)\n",
    "            i += 1\n",
    "# We could edit this slightly so that it saved the epoch number and not just the count of how many images are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53f67f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epoch, model):\n",
    "    output_path = 'model_output'\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    filename = os.path.join(output_path, f\"model_at_epoch-{epoch}.joblib\")\n",
    "    joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa9a793",
   "metadata": {},
   "source": [
    "## Compiling\n",
    "\n",
    "The next thing we need to do is compile our models using desired optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c05fade1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 4096)              413696    \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " up_sampling2d_5 (UpSampling  (None, 8, 8, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 8, 8, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " up_sampling2d_6 (UpSampling  (None, 16, 16, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 16, 16, 256)      1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " up_sampling2d_7 (UpSampling  (None, 32, 32, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 32, 32, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 32, 32, 256)      1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 32, 32, 256)       0         \n",
      "                                                                 \n",
      " up_sampling2d_8 (UpSampling  (None, 64, 64, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 64, 64, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 64, 64, 256)      1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 64, 64, 256)       0         \n",
      "                                                                 \n",
      " up_sampling2d_9 (UpSampling  (None, 128, 128, 256)    0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 128, 128, 256)     590080    \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 128, 128, 256)    1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 128, 128, 256)     0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,369,216\n",
      "Trainable params: 3,366,656\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_shape = (IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS) # (128,128,3)\n",
    "\n",
    "# We use adam optimization\n",
    "optimizer = Adam(learning_rate = 1.5e-4,beta_1 = 0.5)\n",
    "\n",
    "# Simple discriminator loss since binary classifier and metric is accuracy since we are looking for accurate results.\n",
    "discriminator = build_discriminator(image_shape)\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# for the generator we combine it with the disciminator before compiling \n",
    "\n",
    "generator = build_generator(NOISE_SIZE, IMAGE_CHANNELS)\n",
    "\n",
    "random_input = Input(shape=(NOISE_SIZE,))\n",
    "generated_image = generator(random_input)\n",
    "validity = discriminator(generated_image)\n",
    "\n",
    "combined = Model(random_input, validity)\n",
    "\n",
    "combined.compile(loss='binary_crossentropy',\n",
    "optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0d7f78",
   "metadata": {},
   "source": [
    "## Running Code \n",
    "\n",
    "Finally we can run our GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44f01872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resuffling data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x1433e41f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/saulwegrzyn/.pyenv/versions/3.8.12/envs/gandy_warhol/lib/python3.8/site-packages/tqdm/std.py\", line 1162, in __del__\n",
      "    self.close()\n",
      "  File \"/Users/saulwegrzyn/.pyenv/versions/3.8.12/envs/gandy_warhol/lib/python3.8/site-packages/tqdm/notebook.py\", line 286, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch, Discriminator accuracy: 100.0, Generator accuracy: 100.0\n",
      "resuffling data...\n",
      "resuffling data...\n",
      "resuffling data...\n"
     ]
    }
   ],
   "source": [
    "# first we set our y, simply 1 for real images and 0 for fake images\n",
    "y_real = np.ones((BATCH_SIZE, 1))\n",
    "y_fake = np.zeros((BATCH_SIZE, 1))\n",
    "\n",
    "# we freeze our discriminator (note it it still being trained in the combine)\n",
    "discriminator.trainable = False\n",
    "\n",
    "# instantiate 28 images of noise since we are creating 28 images with each predict \n",
    "# this noise stays fixed so that each image in each position is generated from same noise \n",
    "fixed_noise = np.random.normal(0, 1, (PREVIEW_ROWS * PREVIEW_COLS, NOISE_SIZE))\n",
    "cnt = 1\n",
    "for epoch in range(EPOCHS):\n",
    "    for index in range(len(train_set)): # note each iteration of for loop is one batch \n",
    "        x_real = train_set[index]/255. # set image between (0,1)\n",
    "        noise = np.random.normal(0, 1, (BATCH_SIZE, NOISE_SIZE)) # this is the noise to test our metric so isnt fixed\n",
    "        x_fake = generator.predict(noise) # create fake image\n",
    "        discriminator_metric_real = discriminator.train_on_batch(x_real, y_real) # how good is discrim at real image\n",
    "        discriminator_metric_generated = discriminator.train_on_batch(\n",
    "        x_fake, y_fake) # how good is discrim at detecting fake images\n",
    "        discriminator_metric = 0.5 * np.add(discriminator_metric_real, discriminator_metric_generated)\n",
    "        generator_metric = combined.train_on_batch(noise, y_real) # how good is the generator at trying to produce real images \n",
    "    train_set.on_epoch_end() # shuffle the images so the next epoch will have different batches \n",
    "    if epoch % SAVE_FREQ == 0: # save images at desired intervals \n",
    "        save_images(cnt, fixed_noise, epoch)\n",
    "        cnt += 1\n",
    "        print(f'{epoch} epoch, Discriminator accuracy: {100*  discriminator_metric[1]}, Generator accuracy: {100 * generator_metric[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c21296",
   "metadata": {},
   "source": [
    "Some notes for this notebook:\n",
    "Certain widgets are not installed such as tqdm and plot_model which can be used to visualized in a better way. Also it ran only on 8 images, for 4 epochs with a batch size of 2. \n",
    "The actual code was run on 3566 images, for 400 epochs with a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0296918c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
